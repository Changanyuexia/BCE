{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Changanyuexia/BCE/blob/main/bug_report_vadility_classification_with_BERT_and_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFbMRUsFuc9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUMsJFaZQv8G",
        "outputId": "67ab461a-8013-4097-a6ff-98a7a3f4b19e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcgezZXlnQ79"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daQhsxBmQWWq",
        "outputId": "ee76d7a3-e9f3-4925-8119-f026692e68a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Thunderbird/train_bert_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Thunderbird/test_bert_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Netbeans/train_bert_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Netbeans/test_bert_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Mozilla/train_bert_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Mozilla/test_bert_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Firefox/test_bert_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Firefox/train_bert_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Eclipse/train_bert_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/bert_dataset/Eclipse/test_bert_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Thunderbird/train_cnn_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Thunderbird/test_cnn_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Mozilla/test_cnn_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Mozilla/train_cnn_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Netbeans/train_cnn_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Netbeans/test_cnn_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Firefox/train_cnn_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Firefox/test_cnn_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Ecplise/test_cnn_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Ecplise/train_cnn_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Thunderbird/train_none_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Thunderbird/test_none_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Mozilla/train_none_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Mozilla/test_none_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Netbeans/test_none_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Netbeans/train_none_NETBEANS.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Firefox/train_none_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Firefox/test_none_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Eclipse/train_none_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/none_dataset/Eclipse/test_none_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Thunderbird/test_dup_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Thunderbird/train_dup_THUNDERBIRD.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Eclipse/train_dup_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Eclipse/test_dup_ECLIPSE.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Mozilla/train_dup_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Mozilla/test_dup_MOZILLA.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Firefox/train_dup_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/Firefox/test_dup_FIREFOX.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/JDT/train_dup_JDT.csv\n",
            "/content/drive/MyDrive/Validity dataset/dup_dataset/JDT/test_dup_JDT.csv\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Validity dataset'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRGjoO4iVmb6",
        "outputId": "6fc907f0-4db2-47d4-991a-e20e51a1834d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Mozilla/test_cnn_MOZILLA.csv\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "path = \"/content/drive/MyDrive/Validity dataset\"\n",
        "#dataset_type: cnn, bert\n",
        "dataset_type = \"cnn\"\n",
        "#project: Thunderbird, Netbeans, Mozilla, Firefox, Eclipse\n",
        "project = \"Mozilla\"\n",
        "\n",
        "invalid_file_name = \"/content/drive/MyDrive/Validity dataset\" + \"/\" + dataset_type + \"_dataset\" + \"/\" + project + \"/\" + \"invalid_\" + dataset_type + \"_\" + project.upper() + \".csv\"\n",
        "valid_file_name = \"/content/drive/MyDrive/Validity dataset\" + \"/\" + dataset_type + \"_dataset\" + \"/\" + project + \"/\" + \"valid_\" + dataset_type + \"_\" + project.upper() + \".csv\"\n",
        "train_file_name = \"/content/drive/MyDrive/Validity dataset\" + \"/\" + dataset_type + \"_dataset\" + \"/\" + project + \"/\" + \"train_\" + dataset_type + \"_\" + project.upper() + \".csv\"\n",
        "test_file_name = \"/content/drive/MyDrive/Validity dataset\" + \"/\" + dataset_type + \"_dataset\" + \"/\" + project + \"/\" + \"test_\" + dataset_type + \"_\" + project.upper() + \".csv\"\n",
        "print(test_file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvPdVsq0QWWu",
        "outputId": "f3fd2db3-24fa-4e91-ce62-3be7a6d1ac72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Validity dataset/cnn_dataset/Mozilla/train_cnn_MOZILLA.csv\n"
          ]
        }
      ],
      "source": [
        "print(train_file_name)\n",
        "train_df = pd.read_csv(train_file_name)\n",
        "test_df = pd.read_csv(test_file_name)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "train_df['text'] = train_df['text'].astype(str).tolist()\n",
        "test_df['text'] = test_df['text'].astype(str).tolist()\n",
        "\n",
        "train_df['description'] = train_df.apply(lambda row: str(row['title']) + str(row['text']), axis=1)\n",
        "test_df['description'] = test_df.apply(lambda row: str(row['title']) + str(row['text']), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWuZETyatem2"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMMYF9-XtZje",
        "outputId": "01e29cf3-80d4-45a4-83b0-2d6ca2c9a18e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ4BzP2__lRZ"
      },
      "outputs": [],
      "source": [
        "# Convert 'description' and 'valid' columns to numpy array\n",
        "data = pd.concat([pd.read_csv(train_file_name), pd.read_csv(test_file_name)], ignore_index=True)\n",
        "data[\"description\"] = data[\"title\"].str.cat(data[\"text\"], sep=\" \").astype(str)\n",
        "# 获取连接后的文本和标签\n",
        "texts = np.array(data[\"description\"])\n",
        "labels = np.array(data[\"valid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzOCvcpvuQgf",
        "outputId": "1bcebd24-4667-4c4d-a81c-254cd7cdff74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=fbac93cfebd1cb09ee2fff01b601e6fe703c4315e55d6c65206d36dfa512f4a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmXRAIO6uYBu",
        "outputId": "7a2adf80-5ed8-48ee-ffb6-ebcc5a5aee67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-24 09:51:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.248.124, 65.8.248.22, 65.8.248.127, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.248.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘fastText/crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G   146MB/s    in 10s     \n",
            "\n",
            "2023-06-24 09:52:10 (141 MB/s) - ‘fastText/crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n",
            "Archive:  fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: fastText/crawl-300d-2M.vec  \n",
            "CPU times: user 379 ms, sys: 44.9 ms, total: 424 ms\n",
            "Wall time: 49.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT_3Zmh1uaVo",
        "outputId": "e4fba8d8-337a-435b-e6db-f759ce79de2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjmxurHPugR6"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(texts):\n",
        "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
        "\n",
        "    Args:\n",
        "        texts (List[str]): List of text data\n",
        "\n",
        "    Returns:\n",
        "        tokenized_texts (List[List[str]]): List of list of tokens\n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        max_len (int): Maximum sentence length\n",
        "    \"\"\"\n",
        "\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    # Add <pad> and <unk> tokens to the vocabulary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    # Building our vocab from the corpus starting from index 2\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "\n",
        "        # Add `tokenized_sent` to `tokenized_texts`\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        # Add new token to `word2idx`\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
        "    their index in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
        "            shape (N, max_len). It will the input of our CNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Truncate or pad sentences to max_len\n",
        "        if len(tokenized_sent) > max_len:\n",
        "            tokenized_sent = tokenized_sent[:max_len]\n",
        "        else:\n",
        "            tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "\n",
        "    return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8A_-jq7ujzF"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    \"\"\"Load pretrained vectors and create embedding layers.\n",
        "\n",
        "    Args:\n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        fname (str): Path to pretrained vector file\n",
        "\n",
        "    Returns:\n",
        "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
        "            the size of word2idx and d is embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading pretrained vectors...\")\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    # Initilize random embeddings\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    # Load pretrained vectors\n",
        "    count = 0\n",
        "    for line in tqdm_notebook(fin):\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqOR1P_VtEKf",
        "outputId": "7864c39c-434a-45c1-de43-a6c8eae4fa0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzCK8fFxyxE7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "6c29bdc037334ddfb9869faad856f72e",
            "1cb90bf4049a4b79929e514bf69d1211",
            "0e2913cda3d5439587bc2b8e4b02300a",
            "c1f0829809db452da89549856bb6aabd",
            "6c1c3ba691e04d66a934d2c9201c6f7c",
            "3ef74749b7fd468383bd7c07a5ad6204",
            "67a3d593ace942bd8923b4d87b5e43ba",
            "03b52af3df124ed6bd170357aec80fd8",
            "e4bece6d78984b28ad92e4b6c5f8d390",
            "c027fdcccb1c4bf98e83364b5f7817b9",
            "9ceee4a49c26447c88aa20af3fc387f2"
          ]
        },
        "id": "0uwHxpyDumVy",
        "outputId": "acf3e665-d709-43eb-e9a7-4e56db9dba66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing...\n",
            "\n",
            "Loading pretrained vectors...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-af6be4a0c69a>:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for line in tqdm_notebook(fin):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c29bdc037334ddfb9869faad856f72e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 4604 / 410683 pretrained vectors found.\n"
          ]
        }
      ],
      "source": [
        "# # from sentence_transformers import SentenceTransformer\n",
        "# # Tokenize, build vocabulary, encode tokens\n",
        "print(\"Tokenizing...\\n\")\n",
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, 512)\n",
        "\n",
        "# Load pretrained vectors\n",
        "embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n",
        "embeddings = torch.tensor(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlvTKoLCus_z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
        "                              SequentialSampler)\n",
        "\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
        "                batch_size=50):\n",
        "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
        "    DataLoader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data type to torch.Tensor\n",
        "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
        "    tuple(torch.tensor(data) for data in\n",
        "          [train_inputs, val_inputs, train_labels, val_labels])\n",
        "\n",
        "    # Specify batch_size\n",
        "    batch_size = 50\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Create DataLoader for validation data\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygPUBuTauuk6",
        "outputId": "e6fbc8de-feab-442e-beb2-5929d364cb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "# Train Test Split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    input_ids, labels, test_size=0.1, shuffle = False, stratify = None)\n",
        "print(val_labels)\n",
        "\n",
        "\n",
        "# Load data to PyTorch DataLoader\n",
        "train_dataloader, val_dataloader = \\\n",
        "data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJCg1jSZuzmB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_NLP(nn.Module):\n",
        "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        The constructor for CNN_NLP class.\n",
        "\n",
        "        Args:\n",
        "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
        "                shape (vocab_size, embed_dim)\n",
        "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
        "                vectors. Default: False\n",
        "            vocab_size (int): Need to be specified when not pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100]\n",
        "            n_classes (int): Number of classes. Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_NLP, self).__init__()\n",
        "        # Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            # pretrained_embedding = torch.from_numpy(pretrained_embedding)\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "\n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "\n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvMZfCIbu3dF"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def initilize_model(pretrained_embedding=None,\n",
        "                    freeze_embedding=False,\n",
        "                    vocab_size=None,\n",
        "                    embed_dim=300,\n",
        "                    filter_sizes=[3, 4, 5],\n",
        "                    num_filters=[100, 100, 100],\n",
        "                    num_classes=2,\n",
        "                    dropout=0.5,\n",
        "                    learning_rate=0.01):\n",
        "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
        "\n",
        "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
        "    num_filters need to be of the same length.\"\n",
        "\n",
        "    # Instantiate CNN model\n",
        "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
        "                        freeze_embedding=freeze_embedding,\n",
        "                        vocab_size=vocab_size,\n",
        "                        embed_dim=embed_dim,\n",
        "                        filter_sizes=filter_sizes,\n",
        "                        num_filters=num_filters,\n",
        "                        num_classes=2,\n",
        "                        dropout=0.5)\n",
        "\n",
        "    # Send model to `device` (GPU/CPU)\n",
        "    cnn_model.to(device)\n",
        "\n",
        "    # Instantiate Adadelta optimizer\n",
        "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
        "                               lr=learning_rate,\n",
        "                               rho=0.95)\n",
        "\n",
        "    return cnn_model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEJzuRuMu5sz",
        "outputId": "19637eed-d35f-4825-dbfa-8d1387cb3df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install scikit-learn\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHqryfoVu-Dx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"Evaluate the model on the validation set.\"\"\"\n",
        "    # Put the model into the evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "    invalid_precision = []\n",
        "    invalid_recall = []\n",
        "    invalid_f1 = []\n",
        "    valid_precision = []\n",
        "    valid_recall = []\n",
        "    valid_f1 = []\n",
        "\n",
        "    # For each batch in the validation set\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(b_labels.cpu(), preds.cpu(), average=None)\n",
        "\n",
        "        invalid_precision.append(precision[0])\n",
        "        invalid_recall.append(recall[0])\n",
        "        invalid_f1.append(f1[0])\n",
        "        if len(precision) > 1:\n",
        "            valid_precision.append(precision[1])\n",
        "            valid_recall.append(recall[1])\n",
        "            valid_f1.append(f1[1])\n",
        "        else:\n",
        "            valid_precision.append(0)\n",
        "            valid_recall.append(0)\n",
        "            valid_f1.append(0)\n",
        "\n",
        "    # Compute the average loss, accuracy, precision, recall, and F1-score over the validation set\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "    invalid_precision = np.mean(invalid_precision)\n",
        "    invalid_recall = np.mean(invalid_recall)\n",
        "    invalid_f1 = np.mean(invalid_f1)\n",
        "    valid_precision = np.mean(valid_precision)\n",
        "    valid_recall = np.mean(valid_recall)\n",
        "    valid_f1 = np.mean(valid_f1)\n",
        "\n",
        "    return val_loss, val_accuracy, invalid_precision, invalid_recall, invalid_f1, valid_precision, valid_recall, valid_f1\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    set_seed()\n",
        "\n",
        "    # Tracking best validation accuracy\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Invalid Precision':^17} | {'Invalid Recall':^16} | {'Invalid F1':^12} | {'Valid Precision':^17} | {'Valid Recall':^16} | {'Valid F1':^12} | {'Elapsed':^9}\")\n",
        "    print(\"-\" * 125)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # Tracking time and loss\n",
        "        t0_epoch = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if val_dataloader is not None:\n",
        "            # After the completion of each training epoch, measure the model's\n",
        "            # performance on our validation set.\n",
        "            val_loss, val_accuracy, invalid_precision, invalid_recall, invalid_f1, valid_precision, valid_recall, valid_f1 = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Track the best accuracy\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {invalid_precision:^17.2f} | {invalid_recall:^16.2f} | {invalid_f1:^12.2f} | {valid_precision:^17.2f} | {valid_recall:^16.2f} | {valid_f1:^12.2f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf6i5saavA5w",
        "outputId": "be3af6e9-bc92-4a4c-c012-3994ec75b43a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  | Invalid Precision |  Invalid Recall  |  Invalid F1  |  Valid Precision  |   Valid Recall   |   Valid F1   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------------------------------------\n",
            "   1    |   0.529994   |  0.498756  |   76.86   |       0.74        |       0.49       |     0.59     |       0.78        |       0.91       |     0.84     |  284.53  \n",
            "   2    |   0.502226   |  0.492091  |   77.47   |       0.76        |       0.50       |     0.59     |       0.78        |       0.92       |     0.84     |  277.48  \n",
            "   3    |   0.492361   |  0.493405  |   77.30   |       0.67        |       0.65       |     0.65     |       0.82        |       0.84       |     0.83     |  277.46  \n",
            "   4    |   0.483687   |  0.478543  |   78.19   |       0.72        |       0.58       |     0.64     |       0.80        |       0.88       |     0.84     |  277.29  \n",
            "   5    |   0.475221   |  0.478779  |   78.08   |       0.75        |       0.54       |     0.62     |       0.79        |       0.90       |     0.84     |  277.21  \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 78.19%.\n"
          ]
        }
      ],
      "source": [
        "# CNN-rand: Word vectors are randomly initialized.\n",
        "set_seed(42)\n",
        "cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n",
        "                                      embed_dim=300,\n",
        "                                      learning_rate=0.1,\n",
        "                                      dropout=0.5)\n",
        "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzjaDk4zvGTm",
        "outputId": "f0322c67-9363-49a0-d178-ee614d3daefa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  | Invalid Precision |  Invalid Recall  |  Invalid F1  |  Valid Precision  |   Valid Recall   |   Valid F1   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------------------------------------\n",
            "   1    |   0.541288   |  0.509691  |   76.24   |       0.75        |       0.46       |     0.56     |       0.77        |       0.92       |     0.83     |   81.58  \n",
            "   2    |   0.511946   |  0.504204  |   76.71   |       0.77        |       0.45       |     0.56     |       0.76        |       0.93       |     0.84     |   80.93  \n",
            "   3    |   0.503461   |  0.501249  |   77.02   |       0.67        |       0.64       |     0.65     |       0.82        |       0.84       |     0.83     |   80.76  \n",
            "   4    |   0.497801   |  0.487955  |   77.51   |       0.73        |       0.55       |     0.62     |       0.79        |       0.89       |     0.84     |   80.68  \n",
            "   5    |   0.492830   |  0.487657  |   77.70   |       0.75        |       0.52       |     0.60     |       0.78        |       0.91       |     0.84     |   80.81  \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 77.70%.\n"
          ]
        }
      ],
      "source": [
        "# CNN-static: fastText pretrained word vectors are used and freezed during training.\n",
        "set_seed(42)\n",
        "cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
        "                                        freeze_embedding=True,\n",
        "                                        learning_rate=0.1,\n",
        "                                        dropout=0.5)\n",
        "train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtY8ZIr3QftX"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xmJHWVhxSlQz",
        "outputId": "6cbfb749-964e-4803-9f11-fd6c57b9c668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3_lvIJjyQWWt",
        "outputId": "91299ac8-fe0c-4a52-848c-c98170447833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
        "from transformers import BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import trange\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from transformers import BartModel, BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "96bb0c8152894a9d82a28796d6ca5f9b",
            "2c4a912657934ee9b65523320fa352bb",
            "1db74863a66541d6ab25451fbd5f3a97"
          ]
        },
        "id": "LD-XZ0-uQWWv",
        "outputId": "f4f16e5f-54fd-4f12-a7e1-67a775cb7c1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96bb0c8152894a9d82a28796d6ca5f9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c4a912657934ee9b65523320fa352bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1db74863a66541d6ab25451fbd5f3a97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "\n",
        "# model_name = \"distilbert-base-uncased\" #faster\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U92v8tGzQWWv"
      },
      "outputs": [],
      "source": [
        "# Configuration Settings\n",
        "NUM_LABELS = 2\n",
        "BATCH_SIZE = 8\n",
        "MAX_LEN = 512\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-5\n",
        "# LEARNING_RATE = 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yrnFTkqQQWWv",
        "outputId": "b7e871b4-9dba-4501-a6a0-662c135c0385"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def process_data(df, tokenizer, max_len=30):\n",
        "    \"\"\"\n",
        "    Process the data to feed into the pretrained model\n",
        "    \"\"\"\n",
        "    data_processed = tokenizer.batch_encode_plus(df.description.values, pad_to_max_length=True, truncation=True, add_special_tokens=True, return_attention_mask=True, max_length=max_len, return_tensors='pt')\n",
        "    input_ids_train = data_processed[\"input_ids\"]\n",
        "    attention_mask_train = data_processed[\"attention_mask\"]\n",
        "    output_train = torch.tensor(df.valid.values)\n",
        "    return input_ids_train, attention_mask_train, output_train\n",
        "\n",
        "input_ids_train, attention_mask_train, output_train = process_data(train_df, tokenizer, max_len=MAX_LEN)\n",
        "input_ids_test, attention_mask_test, output_test = process_data(test_df, tokenizer, max_len=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sj2U6iv7QWWv"
      },
      "outputs": [],
      "source": [
        "def load_data(seq, mask, output, batch_size=32, use_train=True):\n",
        "    \"\"\"\n",
        "    Load data into batches\n",
        "    \"\"\"\n",
        "    data = TensorDataset(seq, mask, output)\n",
        "    if use_train: sampler = RandomSampler(data)\n",
        "    else: sampler = SequentialSampler(data)\n",
        "    dl_data = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    return dl_data\n",
        "\n",
        "dl_train = load_data(input_ids_train, attention_mask_train, output_train, batch_size=BATCH_SIZE)\n",
        "dl_test = load_data(input_ids_test, attention_mask_test, output_test, batch_size=BATCH_SIZE, use_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "be092f2074824eee8edad8dc16652bd6"
          ]
        },
        "id": "nQUCSZYMQWWv",
        "outputId": "922123d6-f77c-463d-d717-d9142c7f9233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be092f2074824eee8edad8dc16652bd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #train on gpu if available\n",
        "print(\"Using device {}.\\n\".format(device))\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=NUM_LABELS, output_hidden_states=False, output_attentions=False)\n",
        "# model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=NUM_LABELS, output_hidden_states=False, output_attentions=False)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=LEARNING_RATE)  # Default optimization\n",
        "# Learning rate scheduling is applied after optimizer’s update to adjust the learning rate based on the number of epochs.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dl_train) * EPOCHS) #number of batches * number of epochs\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLEk3UoYTkLA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u3d5uLoEQWWw",
        "outputId": "1b474f80-ba44-4c65-9c39-8e4995b52496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has total 66955010 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    ''' Count the total number of trainable parameters '''\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(\"The model has total {} trainable parameters\".format(num_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1EaHLOHgQWWw"
      },
      "outputs": [],
      "source": [
        "def eval_metric(predictions, labels):\n",
        "  ''' Calculate average accuracy of the data samples '''\n",
        "  max_predictions = predictions.argmax(axis=1, keepdim=True) #[batch_size]\n",
        "  avg_acc = round(accuracy_score(y_true=labels.to('cpu').tolist(), y_pred=max_predictions.detach().cpu().numpy()), 2)*100\n",
        "  return avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nIq0BfIdQWWw"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, train_loader, optimizer, device, scheduler, criterion=None):\n",
        "  ''' Define the training function '''\n",
        "  model.train() #set the model on train mode\n",
        "  total_loss, total_acc = 0, 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    batch = tuple(item.to(device) for item in batch)\n",
        "    input_ids, input_mask, labels = batch\n",
        "    optimizer.zero_grad() # clear gradients from last batch\n",
        "    outputs = model(input_ids, attention_mask=input_mask, labels=labels) # get predictions\n",
        "#     loss = criterion(batch_y_pred, batch_y) # compute the loss\n",
        "    loss = outputs.loss\n",
        "    total_loss += loss.item() #aggregate the losses\n",
        "    loss.backward() # compute the gradients\n",
        "    optimizer.step() # update the parameters\n",
        "    scheduler.step()\n",
        "    logits = outputs.logits\n",
        "    total_acc += eval_metric(logits, labels)\n",
        "\n",
        "  loss_per_epoch = total_loss/len(train_loader)\n",
        "  acc_per_epoch = total_acc/len(train_loader)\n",
        "  return loss_per_epoch, acc_per_epoch\n",
        "\n",
        "def eval_fn(model, data_loader, device, criterion=None):\n",
        "  ''' Define the evaluation function '''\n",
        "  model.eval() # set the model on eval mode\n",
        "  total_loss, total_acc = 0, 0\n",
        "\n",
        "  with torch.no_grad(): # do not need to update the parameters\n",
        "    for batch in data_loader:\n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      input_ids, input_mask, labels = batch\n",
        "      outputs = model(input_ids, attention_mask=input_mask, labels=labels) # get predictions\n",
        "#       loss = criterion(batch_y_pred, batch_y) # compute the loss\n",
        "      loss = outputs.loss\n",
        "      total_loss += loss.item() #aggregate the losses\n",
        "      logits = outputs.logits\n",
        "      total_acc += eval_metric(logits, labels)\n",
        "\n",
        "  loss_per_epoch = total_loss/len(data_loader)\n",
        "  acc_per_epoch = total_acc/len(data_loader)\n",
        "  return loss_per_epoch, acc_per_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FkREotX2H-Fk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "73p6oXnHTk8K",
        "outputId": "2ece4f5c-7f38-41ab-811e-1b2fef604aaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from tqdm import trange\n",
        "\n",
        "# Training\n",
        "train_losses, test_losses = [], [] # Store train and validation losses for plotting\n",
        "train_accuracies, test_accuracies = [], []\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in trange(EPOCHS, desc=\"Epoch\"):\n",
        "  train_loss_per_epoch, train_acc_per_epoch = train_fn(model, dl_train, optimizer, device, scheduler)\n",
        "  test_loss_per_epoch, test_acc_per_epoch = eval_fn(model, dl_test, device)\n",
        "\n",
        "  train_losses.append(train_loss_per_epoch)\n",
        "  test_losses.append(test_loss_per_epoch)\n",
        "  train_accuracies.append(train_acc_per_epoch)\n",
        "  test_accuracies.append(test_acc_per_epoch)\n",
        "\n",
        "  if test_loss_per_epoch < best_test_loss:\n",
        "    best_test_loss = test_loss_per_epoch\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/model.pt')\n",
        "\n",
        "  print(\"Epoch: {}, Train Loss: {:.4f}, Train Accuracy: {:.2f}%\".format(epoch, train_loss_per_epoch, train_acc_per_epoch))\n",
        "  print(\"Epoch: {}, test Loss: {:.4f}, test Accuracy: {:.2f}%\\n\".format(epoch, test_loss_per_epoch, test_acc_per_epoch)) # Test\n",
        "\n",
        "# Loading the best trained model\n",
        "print(\"Loading the best trained model\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model.pt'))\n",
        "\n",
        "# Create a DataLoader for the test data\n",
        "test_dataset = TensorDataset(input_ids_test, attention_mask_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)  # Adjust batch_size to fit your GPU\n",
        "\n",
        "preds = []\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Predict in batches\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        logits = outputs.logits\n",
        "        batch_preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "        preds.extend(batch_preds)\n",
        "\n",
        "print(classification_report(output_test, preds))\n",
        "print(\"ROC AUC Score: {}\".format(roc_auc_score(y_true=output_test, y_score=preds)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "ec9pUc2jIDP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer, return_all_scores=True , device=0)"
      ],
      "metadata": {
        "id": "_KzfvUJLJ0HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME"
      ],
      "metadata": {
        "id": "M2qYFWu0IFcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "fMcL03d7J3xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_text\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "class_labels = [\"invalid\", \"valid\"]\n",
        "lime_explainer = LimeTextExplainer(class_names=class_labels)"
      ],
      "metadata": {
        "id": "glqdqnuVJ6Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba_fn(texts):\n",
        "    a=pipe(texts)\n",
        "    rt=[]\n",
        "    for i in a:\n",
        "        prb=[]\n",
        "        for j in i:\n",
        "            prb.append(j['score'])\n",
        "        rt.append(prb)\n",
        "    return np.array(rt)"
      ],
      "metadata": {
        "id": "iyQ_XI96J8MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"[patch] To add a File->New->Window menuitem and remove doubleclick in folder pane opens a new window. Currently, double clicking on something in the folder pane opens a new window with that folder selected.  To me, this is bad UI, as opening a new window is an expensive operation and not something most people are going to want.  In addition to this, the new window doesnt provide a different UI at all, its just a duplicate,  so basically you double click and you get two copies of the exact same window. This patch removes the opening of a new window on double click, and adds a new menuitem that opens a new window with the current folder selected.  I chose F5 for the accelerator key, but if someone feels something different is better, by all means.  Also, if for some reason the double click to open makes sense in nntp account (I dont have one, so I dont know),  I can easily add a serverType check to restrict the no new-window behaviour to imap and pop3 accounts.\n",
        "Reproducible: Always.\n",
        "Steps to Reproduce:  1. Double click a folder.\n",
        "Actual Results: new window is opened with the same ui as the original window.\n",
        "Expected Results: nothing\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TNxlkdRPKCLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the LIME explainer to generate an explanation\n",
        "exp = lime_explainer.explain_instance(text, predict_proba_fn)\n",
        "\n",
        "# Display the explanation visualization in the notebook, with text included\n",
        "exp.show_in_notebook(text=True)"
      ],
      "metadata": {
        "id": "cXY_fJ7rKGU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP"
      ],
      "metadata": {
        "id": "h8aMJwyqIH0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import scipy as sp\n",
        "\n",
        "def f(x):\n",
        "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=500, truncation=True) for v in x]).cuda()\n",
        "    outputs = model(tv)[0].detach().cpu().numpy()\n",
        "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
        "    val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n",
        "    return val\n",
        "\n",
        "# build an explainer using a token masker\n",
        "explainer = shap.Explainer(f, tokenizer)\n",
        "shap_values = explainer([text])\n",
        "\n",
        "# plot the explanation\n",
        "shap.plots.text(shap_values)\n"
      ],
      "metadata": {
        "id": "TpqU7Z0VOIVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Intepret"
      ],
      "metadata": {
        "id": "_OSt53TwJwP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "\n",
        "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "attributions = cls_explainer(text)\n",
        "\n",
        "# Iterate through the list of attributions\n",
        "for word, score in attributions:\n",
        "    print(f\"Word: {word} - Score: {score}\")\n",
        "\n",
        "# Get predicted class\n",
        "predicted_class_name = cls_explainer.predicted_class_name\n",
        "\n",
        "print(f\"Predicted class name: {predicted_class_name}\")\n",
        "\n",
        "# Visualize explanation for the first class\n",
        "cls_explainer.visualize(\"trans_389374.html\")"
      ],
      "metadata": {
        "id": "uKsIfBnONtiv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03b52af3df124ed6bd170357aec80fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0e2913cda3d5439587bc2b8e4b02300a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b52af3df124ed6bd170357aec80fd8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4bece6d78984b28ad92e4b6c5f8d390",
            "value": 1
          }
        },
        "1cb90bf4049a4b79929e514bf69d1211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef74749b7fd468383bd7c07a5ad6204",
            "placeholder": "​",
            "style": "IPY_MODEL_67a3d593ace942bd8923b4d87b5e43ba",
            "value": ""
          }
        },
        "3ef74749b7fd468383bd7c07a5ad6204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a3d593ace942bd8923b4d87b5e43ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c1c3ba691e04d66a934d2c9201c6f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c29bdc037334ddfb9869faad856f72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cb90bf4049a4b79929e514bf69d1211",
              "IPY_MODEL_0e2913cda3d5439587bc2b8e4b02300a",
              "IPY_MODEL_c1f0829809db452da89549856bb6aabd"
            ],
            "layout": "IPY_MODEL_6c1c3ba691e04d66a934d2c9201c6f7c"
          }
        },
        "9ceee4a49c26447c88aa20af3fc387f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c027fdcccb1c4bf98e83364b5f7817b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f0829809db452da89549856bb6aabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c027fdcccb1c4bf98e83364b5f7817b9",
            "placeholder": "​",
            "style": "IPY_MODEL_9ceee4a49c26447c88aa20af3fc387f2",
            "value": " 1999995/? [00:31&lt;00:00, 66199.34it/s]"
          }
        },
        "e4bece6d78984b28ad92e4b6c5f8d390": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}